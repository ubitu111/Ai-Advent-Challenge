#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π Whisper —Å–µ—Ä–≤–µ—Ä —Å OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º API (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç faster-whisper)
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python whisper_server_faster.py
"""

from fastapi import FastAPI, File, UploadFile, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from faster_whisper import WhisperModel
import tempfile
import os
import sys

app = FastAPI(title="Whisper API Server (Faster)")

# –î–æ–±–∞–≤–∏—Ç—å CORS –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –º–æ–¥–µ–ª–∏
model = None

def load_model(model_name: str = "base"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Whisper (faster-whisper)"""
    global model
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper: {model_name}...")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º faster-whisper –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # device="cpu" –¥–ª—è CPU, "cuda" –¥–ª—è GPU
        # compute_type="int8" –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞ CPU
        device = os.getenv("WHISPER_DEVICE", "cpu")
        compute_type = os.getenv("WHISPER_COMPUTE_TYPE", "int8")
        
        model = WhisperModel(model_name, device=device, compute_type=compute_type)
        print(f"–ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! (device={device}, compute_type={compute_type})")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: tiny, base, small, medium, large-v3")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ faster-whisper —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install faster-whisper")
        sys.exit(1)

@app.on_event("startup")
async def startup_event():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    model_name = os.getenv("WHISPER_MODEL", "base")
    load_model(model_name)

@app.post("/v1/audio/transcriptions")
async def transcribe_audio(
    file: UploadFile = File(...),
    model_name: str = Form("whisper-1"),
    language: str = Form(None)
):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ —Ñ–∞–π–ª
    –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å OpenAI Whisper API
    """
    try:
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        file_extension = os.path.splitext(file.filename)[1] or ".wav"
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é faster-whisper
        print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {file.filename}")
        segments, info = model.transcribe(
            tmp_path,
            language=language,
            beam_size=5
        )
        
        # –°–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        text = " ".join([segment.text for segment in segments])
        
        # –£–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.unlink(tmp_path)
        
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text[:100]}...")  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
        
        return JSONResponse(content={"text": text})
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "server_error"}}
        )

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "ok", "model_loaded": model is not None}

@app.get("/v1/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å OpenAI API)"""
    return {
        "data": [
            {
                "id": "whisper-1",
                "object": "model",
                "created": 1677610602,
                "owned_by": "openai"
            }
        ],
        "object": "list"
    }

if __name__ == "__main__":
    import uvicorn
    import socket
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø–æ—Ä—Ç–∞
    def is_port_available(port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(('0.0.0.0', port))
                return True
            except OSError:
                return False
    
    # –ü–æ—Ä—Ç –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    default_port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç–∞
    port = default_port
    if not is_port_available(port):
        print(f"‚ö†Ô∏è  –ü–æ—Ä—Ç {port} —É–∂–µ –∑–∞–Ω—è—Ç, –ø—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä—Ç—ã...")
        for alt_port in [8001, 8002, 8003, 8080, 8888]:
            if is_port_available(alt_port):
                port = alt_port
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é –ø–æ—Ä—Ç {port}")
                break
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –ø–æ—Ä—Ç—É {default_port}")
            print(f"   –ò–ª–∏ —É–∫–∞–∂–∏—Ç–µ –¥—Ä—É–≥–æ–π –ø–æ—Ä—Ç: PORT=8001 python whisper_server_faster.py")
            sys.exit(1)
    
    print(f"–ó–∞–ø—É—Å–∫ Whisper API —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ http://{host}:{port}")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
    print(f"\nüí° –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –ø–æ—Ä—Ç, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
    print(f"   PORT=8001 python whisper_server_faster.py")
    
    uvicorn.run(app, host=host, port=port)
