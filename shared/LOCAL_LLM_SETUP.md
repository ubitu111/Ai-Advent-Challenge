# Инструкция по использованию локальной модели Ollama

## Быстрый старт

### 1. Установка Ollama

```bash
# macOS
brew install ollama
# или скачать с https://ollama.com/download/mac
```

### 2. Запуск Ollama и загрузка модели

```bash
# Запустить сервер (в фоне или отдельном терминале)
ollama serve

# В другом терминале скачать модель llama3.1:8b
ollama pull llama3.1:8b
```

### 3. Настройка проекта

Добавьте в файл `local.properties` (или в `api.properties` после сборки) следующие настройки:

```properties
# Включить использование локальной модели
local.model.enabled=true

# URL Ollama сервера (по умолчанию http://localhost:11434)
local.model.base.url=http://localhost:11434

# Имя модели (по умолчанию llama3.1:8b)
local.model.name=llama3.1:8b
```

### 4. Проверка работы

```bash
# Проверить доступность API
curl http://localhost:11434/api/tags

# Протестировать модель
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [
      {"role": "user", "content": "Привет! Как дела?"}
    ],
    "stream": false
  }'
```

## Переключение между моделями

### Использование локальной модели

Установите `local.model.enabled=true` в конфигурации.

### Использование Yandex Pro API

Установите `local.model.enabled=false` в конфигурации или удалите эту настройку.

## Настройка других моделей

Вы можете использовать другие модели Ollama, изменив `local.model.name`:

```properties
local.model.name=mistral:7b
local.model.name=deepseek-coder:7b
local.model.name=phi3:mini
```

Сначала убедитесь, что модель загружена:
```bash
ollama pull <имя_модели>
```

## Примечания

- Локальная модель работает только когда Ollama сервер запущен
- Качество ответов может отличаться от Yandex Pro API
- Некоторые функции (например, tool calling) могут работать по-разному в зависимости от модели
- Для лучшей производительности рекомендуется использовать квантованные версии моделей

## Устранение проблем

### Модель не отвечает

1. Проверьте, что Ollama сервер запущен:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. Убедитесь, что модель загружена:
   ```bash
   ollama list
   ```

3. Проверьте логи приложения на наличие ошибок подключения

### Медленная работа

- Используйте более легкие модели (например, `phi3:mini` вместо `llama3.1:8b`)
- Убедитесь, что у вас достаточно RAM (рекомендуется минимум 8GB для llama3.1:8b)
- Закройте другие ресурсоемкие приложения

### Ошибки подключения

- Проверьте, что `local.model.base.url` указывает на правильный адрес
- Убедитесь, что порт 11434 не заблокирован файрволом
- Попробуйте использовать `http://127.0.0.1:11434` вместо `http://localhost:11434`
